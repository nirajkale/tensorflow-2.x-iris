{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRIS-classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHPv0T0lLGsr",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we'll be doing flower- IRIS classification. This dataset purely is numerical & we'll be exclusively using tensorflow 2.x's newest dataset api (tfds) which is a wrapper around tf.data api. Though it has a bit of a learning curve (as opposed to using keras' functional api + fit method), we'll still try explore it for its clear advantage in execution speed. Furthermore, we'll be using tf 2.x's eager execution feature in the model training itself so as to get the gradient descent out of the black box & to get finer control over the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT5qZErlLHOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, optimizers, regularizers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqxAJpVmh2Fh",
        "colab_type": "text"
      },
      "source": [
        "Once the required libraries are loaded, we can import the iris dataset.\n",
        "Iris is an single-label numerical classification dataset with 3 classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ICltaDriG0j",
        "colab_type": "code",
        "outputId": "d069dc4a-e69f-4017-ed4b-d469629e4c35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "ds, info = tfds.load(\"iris\", split=tfds.Split.TRAIN, with_info = True)\n",
        "print(info)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='iris',\n",
            "    version=2.0.0,\n",
            "    description='This is perhaps the best known database to be found in the pattern recognition\n",
            "literature. Fisher's paper is a classic in the field and is referenced\n",
            "frequently to this day. (See Duda & Hart, for example.) The data set contains\n",
            "3 classes of 50 instances each, where each class refers to a type of iris\n",
            "plant. One class is linearly separable from the other 2; the latter are NOT\n",
            "linearly separable from each other.\n",
            "',\n",
            "    homepage='https://archive.ics.uci.edu/ml/datasets/iris',\n",
            "    features=FeaturesDict({\n",
            "        'features': Tensor(shape=(4,), dtype=tf.float32),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),\n",
            "    }),\n",
            "    total_num_examples=150,\n",
            "    splits={\n",
            "        'train': 150,\n",
            "    },\n",
            "    supervised_keys=('features', 'label'),\n",
            "    citation=\"\"\"@misc{Dua:2019 ,\n",
            "    author = \"Dua, Dheeru and Graff, Casey\",\n",
            "    year = \"2017\",\n",
            "    title = \"{UCI} Machine Learning Repository\",\n",
            "    url = \"http://archive.ics.uci.edu/ml\",\n",
            "    institution = \"University of California, Irvine, School of Information and Computer Sciences\"\n",
            "    }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vIurfv95Fno",
        "colab_type": "text"
      },
      "source": [
        "As per above information, there are 150 samples where each sample has 4 features & labels with 3 classes. Now apperently there is no VALIDATION & TEST split available for this dataset in TFDS. so we have to split the training split itself (In TFDS parts of the dataset are called SPLIT) into 2 set: Training & Test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz6_kmyf4qHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds_train, info_train = tfds.load(\"iris\", split='train[:80%]', with_info = True)\n",
        "ds_val, info_val = tfds.load(\"iris\", split='train[-20%:]', with_info = True)\n",
        "#print(len(list(ds_train.as_numpy_iterator()))) to verfiy the count but shouldn't be used for larger dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gryMbcAummHp",
        "colab_type": "text"
      },
      "source": [
        "unfortunately TF dataset does not have cleaner way to get total length of a split (dataset). Even seeing single sample is little hacky as shown below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rswcrorii2X2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3954764-2194-4178-dc5f-f2e0a70e53c4"
      },
      "source": [
        "sample = next(iter(ds_train.take(1)))\n",
        "print(sample)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'features': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([5.1, 3.4, 1.5, 0.2], dtype=float32)>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNQVobW_nU_v",
        "colab_type": "text"
      },
      "source": [
        "Now, ideally we would be normalizing the numerical features before training but in case of iris dataset, the standard deviation & mean values per column aren't so huge so we can trust the gradient descent to adjust the weights accordingly. Otherwise we would've used tensorflow transform (tft) library for data normalization.\n",
        "Now, irrespective of whether or not we're doing any normalization, we need to write a preprocessing function to extract features & labels from the dataset records (dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKPHO-m8nSHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing_fn(inputs):\n",
        "  print(a)\n",
        "  features= inputs['features']\n",
        "  label = inputs['label']#by referring above info object\n",
        "  return features, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxH6iMD6q6q5",
        "colab_type": "text"
      },
      "source": [
        "Now, preprocssing function will be just the begining of the input pipeline. Next part of the pipeline will be shuffling & batching. So lets create the whole pipeline at once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hMfKAhWnjUa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "202cfd39-4f7a-42e0-f0cd-44b501728757"
      },
      "source": [
        "ds_train, info_train = tfds.load(\"iris\", split='train[:80%]', with_info = True)\n",
        "ds_train = ds_train.map(preprocessing_fn).shuffle(1024).batch(32)\n",
        "ds_val = ds_val.map(preprocessing_fn).batch(32)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.callbacks.History object at 0x7f5ee27c0b00>\n",
            "<tensorflow.python.keras.callbacks.History object at 0x7f5ee27c0b00>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htIDXX-TyAl4",
        "colab_type": "text"
      },
      "source": [
        "The reason, i've re-initialized ds_train is because of some weird issue (not sure if it's an issue) in tfds. Apperantly if we use the take function once, we cannot map the dataset to a preprocessing function.\n",
        "\n",
        "Later in the pipeline, shuffle function in takes buffer size as input & will shuffle the batches in runtime.(we can buffersize ignore it for now as it wont be critical for this dataset). Next we'll add a batching to pipeline so when we iterate over the dataset, we'll by default get data in batches\n",
        "\n",
        "For the sake of performance improvement in larger datasets, you can use the prefetch() function in the pipeline (whcih asynchronously fetches the next batches when the model is busy with training) or you can wait for \"snapshot\" featues in tf 2.3 which would supposedly cache youe pre-processed data in local filesystem.\n",
        "\n",
        "Now lets define the model itself"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9nv5vXttW38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn():\n",
        "  model_in = layers.Input((4,))\n",
        "  a = layers.Dense(12, activation='relu', kernel_regularizer= tf.keras.regularizers.l2(l=0.001))(model_in)\n",
        "  logits = layers.Dense(3)(a)\n",
        "  model= models.Model(model_in, logits)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKh3WL-bOFyn",
        "colab_type": "text"
      },
      "source": [
        "The model is fairly simple with just 1 hidden layer. If you've noticed, in the last layer i've not used softmax function despite this being a classification datset. This is because i need the raw logits this layer to calculate the loss manually. This wont affect our accuracy calculation as logits & probabilities max out at same location ( as-in argmax result for both would be same)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZPA6ORbtf0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "d428b04c-91c9-43f8-8f99-5164f971bda6"
      },
      "source": [
        "model = model_fn()\n",
        "print(model.summary())"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_18 (InputLayer)        [(None, 4)]               0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 12)                60        \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 3)                 39        \n",
            "=================================================================\n",
            "Total params: 99\n",
            "Trainable params: 99\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9r2h6lg6c_w",
        "colab_type": "text"
      },
      "source": [
        "Since we want to use eager execution, we have to write our own custom logic for trainig & validation using something called GradientTape. Now this tape is 2.x only feature meant to compute gardients on the fly. \n",
        "\n",
        "This way of doing training as opposed to simple model.fit may seem like an overkill for this dataset but getting familier with tf.data api & \n",
        "tapes could be perticularly useful when dealing with complex architectures like seq2seq, attention based models etc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGMsWyhM6gGr",
        "colab_type": "text"
      },
      "source": [
        "So in order to use tape, we first need to define the optimizer & a loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V0w1xb6ulE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optimizers.RMSprop()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOU2SCnk8f6b",
        "colab_type": "text"
      },
      "source": [
        "After this we need a training function or more of training step function which will be responsible for forward & backward prop.\n",
        "\n",
        "Also an evaluation function which will calcuate loss & logits for validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWsc_PNz8ed3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(batch_data, labels):\n",
        "  #forward propagation\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(batch_data, training=True)\n",
        "    loss_val = loss_object(labels, logits)\n",
        "  #backpropagation\n",
        "  gradients = tape.gradient(loss_val, model.trainable_variables)\n",
        "  #based on backprop, update model weights\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  metric.update_state(labels, logits)\n",
        "  return loss_val\n",
        "\n",
        "@tf.function\n",
        "def evaluation_step(batch_data):\n",
        "  logits= model(batch_data, training=False)\n",
        "  loss_val = loss_object(labels, logits)\n",
        "  return logits, loss_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvaYgSHAPTbn",
        "colab_type": "text"
      },
      "source": [
        "Notice the @tf.function decorator on top of these functions, this tell tensorflow to create graph for this fucntions. This will improve the speed of training as we want a singleton execution for Forward & backward prop.\n",
        "This decorator can be removed for the sake if you want to debug your model training. If you remove this decorator, you'll also get access to the numpy() values of the tensors like grads, batch_loss etc. Pretty inconvinient Eh ?\n",
        "\n",
        "Also, the reason this training step is defined for the execution of 1 batch is because gradient tapes can only be used once. When instantiated, the record all the operations in forward prop & then these ops are used to calcualte gradients in backward propagation\n",
        "\n",
        "Now, we can write the epoch & batch level training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXeqQQhk8eY1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6599f7d-33d3-48b0-8910-ffcf093001b4"
      },
      "source": [
        "# tf.executing_eagerly()\n",
        "metrics = {\n",
        "    'loss':[],\n",
        "    'acc':[],\n",
        "    'val_loss':[],\n",
        "    'val_acc':[]\n",
        "}\n",
        "for epoch in range(150):\n",
        "  tf.print('Epoch ',epoch,' started')\n",
        "  metric.reset_states()\n",
        "  num_batches = 0\n",
        "  epoch_loss_train = 0\n",
        "  for batch_data, labels in ds_train:\n",
        "    batch_loss = train_step(batch_data, labels)\n",
        "    # batch_loss = batch_loss.numpy().mean()\n",
        "    num_batches += 1\n",
        "    epoch_loss_train += batch_loss\n",
        "  epoch_loss_train /= num_batches\n",
        "  epoch_acc_train = metric.result()\n",
        "  #do the same for validation set\n",
        "  metric.reset_states()\n",
        "  for batch_data, labels in ds_val:\n",
        "    logits, loss_val = evaluation_step(batch_data)\n",
        "    metric.update_state(labels, logits)\n",
        "  epoch_loss_val = loss_val #because we know there's one validation batch\n",
        "  epoch_acc_val = metric.result()\n",
        "  tf.print('Loss {0} | Acc {1} | Validation Loss {2} | Validation Acc {3}'.format(epoch_loss_train, epoch_acc_train, epoch_loss_val, epoch_acc_val))\n",
        "  metrics['loss'].append(epoch_loss_train)\n",
        "  metrics['val_loss'].append(epoch_acc_train)\n",
        "  metrics['acc'].append(epoch_loss_val)\n",
        "  metrics['val_acc'].append(epoch_acc_val)\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0  started\n",
            "Loss 1.2170121669769287 | Acc 0.3333333432674408 | Validation Loss 1.1760237216949463 | Validation Acc 0.20000000298023224\n",
            "Epoch  1  started\n",
            "Loss 1.101813554763794 | Acc 0.23333333432674408 | Validation Loss 1.114747405052185 | Validation Acc 0.23333333432674408\n",
            "Epoch  2  started\n",
            "Loss 1.0629459619522095 | Acc 0.25 | Validation Loss 1.0737850666046143 | Validation Acc 0.20000000298023224\n",
            "Epoch  3  started\n",
            "Loss 1.025876760482788 | Acc 0.30000001192092896 | Validation Loss 1.0448575019836426 | Validation Acc 0.13333334028720856\n",
            "Epoch  4  started\n",
            "Loss 1.0010422468185425 | Acc 0.3583333194255829 | Validation Loss 1.0253793001174927 | Validation Acc 0.30000001192092896\n",
            "Epoch  5  started\n",
            "Loss 0.981864869594574 | Acc 0.3916666805744171 | Validation Loss 1.0062241554260254 | Validation Acc 0.3333333432674408\n",
            "Epoch  6  started\n",
            "Loss 0.9625654220581055 | Acc 0.40833333134651184 | Validation Loss 0.989669680595398 | Validation Acc 0.3333333432674408\n",
            "Epoch  7  started\n",
            "Loss 0.9474779367446899 | Acc 0.4583333432674408 | Validation Loss 0.9729572534561157 | Validation Acc 0.3333333432674408\n",
            "Epoch  8  started\n",
            "Loss 0.9315895438194275 | Acc 0.44999998807907104 | Validation Loss 0.9577168822288513 | Validation Acc 0.4000000059604645\n",
            "Epoch  9  started\n",
            "Loss 0.9196577072143555 | Acc 0.49166667461395264 | Validation Loss 0.9429057836532593 | Validation Acc 0.46666666865348816\n",
            "Epoch  10  started\n",
            "Loss 0.9046745896339417 | Acc 0.5333333611488342 | Validation Loss 0.9282906651496887 | Validation Acc 0.5333333611488342\n",
            "Epoch  11  started\n",
            "Loss 0.887784481048584 | Acc 0.6166666746139526 | Validation Loss 0.9135259985923767 | Validation Acc 0.5333333611488342\n",
            "Epoch  12  started\n",
            "Loss 0.8763560056686401 | Acc 0.625 | Validation Loss 0.89873868227005 | Validation Acc 0.6000000238418579\n",
            "Epoch  13  started\n",
            "Loss 0.8641649484634399 | Acc 0.6583333611488342 | Validation Loss 0.8850104808807373 | Validation Acc 0.6000000238418579\n",
            "Epoch  14  started\n",
            "Loss 0.8457828760147095 | Acc 0.6583333611488342 | Validation Loss 0.8728994131088257 | Validation Acc 0.6666666865348816\n",
            "Epoch  15  started\n",
            "Loss 0.8332785964012146 | Acc 0.699999988079071 | Validation Loss 0.8611946702003479 | Validation Acc 0.6333333253860474\n",
            "Epoch  16  started\n",
            "Loss 0.8192782998085022 | Acc 0.6833333373069763 | Validation Loss 0.8481548428535461 | Validation Acc 0.6666666865348816\n",
            "Epoch  17  started\n",
            "Loss 0.81003737449646 | Acc 0.699999988079071 | Validation Loss 0.835378110408783 | Validation Acc 0.7666666507720947\n",
            "Epoch  18  started\n",
            "Loss 0.7946739792823792 | Acc 0.7250000238418579 | Validation Loss 0.8224363327026367 | Validation Acc 0.7666666507720947\n",
            "Epoch  19  started\n",
            "Loss 0.7794541716575623 | Acc 0.7666666507720947 | Validation Loss 0.8130050301551819 | Validation Acc 0.6666666865348816\n",
            "Epoch  20  started\n",
            "Loss 0.7709845900535583 | Acc 0.7083333134651184 | Validation Loss 0.8004499077796936 | Validation Acc 0.699999988079071\n",
            "Epoch  21  started\n",
            "Loss 0.7562403082847595 | Acc 0.7166666388511658 | Validation Loss 0.7896443605422974 | Validation Acc 0.7666666507720947\n",
            "Epoch  22  started\n",
            "Loss 0.74693763256073 | Acc 0.7333333492279053 | Validation Loss 0.7788122296333313 | Validation Acc 0.7666666507720947\n",
            "Epoch  23  started\n",
            "Loss 0.7318341732025146 | Acc 0.7666666507720947 | Validation Loss 0.7680738568305969 | Validation Acc 0.7333333492279053\n",
            "Epoch  24  started\n",
            "Loss 0.7372223734855652 | Acc 0.7083333134651184 | Validation Loss 0.757111668586731 | Validation Acc 0.7666666507720947\n",
            "Epoch  25  started\n",
            "Loss 0.7156366109848022 | Acc 0.7666666507720947 | Validation Loss 0.7473992109298706 | Validation Acc 0.7666666507720947\n",
            "Epoch  26  started\n",
            "Loss 0.7027829885482788 | Acc 0.7833333611488342 | Validation Loss 0.7380267977714539 | Validation Acc 0.7666666507720947\n",
            "Epoch  27  started\n",
            "Loss 0.6913329362869263 | Acc 0.7749999761581421 | Validation Loss 0.7281322479248047 | Validation Acc 0.7666666507720947\n",
            "Epoch  28  started\n",
            "Loss 0.68437659740448 | Acc 0.8166666626930237 | Validation Loss 0.7187458276748657 | Validation Acc 0.7666666507720947\n",
            "Epoch  29  started\n",
            "Loss 0.6729351878166199 | Acc 0.7833333611488342 | Validation Loss 0.7090829610824585 | Validation Acc 0.800000011920929\n",
            "Epoch  30  started\n",
            "Loss 0.6656134128570557 | Acc 0.7749999761581421 | Validation Loss 0.6989860534667969 | Validation Acc 0.8333333134651184\n",
            "Epoch  31  started\n",
            "Loss 0.6511989235877991 | Acc 0.8500000238418579 | Validation Loss 0.6911669373512268 | Validation Acc 0.7666666507720947\n",
            "Epoch  32  started\n",
            "Loss 0.6448100209236145 | Acc 0.7749999761581421 | Validation Loss 0.6809463500976562 | Validation Acc 0.8333333134651184\n",
            "Epoch  33  started\n",
            "Loss 0.6311946511268616 | Acc 0.8416666388511658 | Validation Loss 0.671853244304657 | Validation Acc 0.8333333134651184\n",
            "Epoch  34  started\n",
            "Loss 0.623348593711853 | Acc 0.8166666626930237 | Validation Loss 0.6628759503364563 | Validation Acc 0.8333333134651184\n",
            "Epoch  35  started\n",
            "Loss 0.617175817489624 | Acc 0.8083333373069763 | Validation Loss 0.6527700424194336 | Validation Acc 0.8333333134651184\n",
            "Epoch  36  started\n",
            "Loss 0.6052912473678589 | Acc 0.8666666746139526 | Validation Loss 0.6445040106773376 | Validation Acc 0.8333333134651184\n",
            "Epoch  37  started\n",
            "Loss 0.5978503823280334 | Acc 0.824999988079071 | Validation Loss 0.635532021522522 | Validation Acc 0.8666666746139526\n",
            "Epoch  38  started\n",
            "Loss 0.5907714366912842 | Acc 0.8916666507720947 | Validation Loss 0.6274183392524719 | Validation Acc 0.8666666746139526\n",
            "Epoch  39  started\n",
            "Loss 0.5784316062927246 | Acc 0.8999999761581421 | Validation Loss 0.6196677088737488 | Validation Acc 0.8333333134651184\n",
            "Epoch  40  started\n",
            "Loss 0.5686050653457642 | Acc 0.8666666746139526 | Validation Loss 0.6113203167915344 | Validation Acc 0.8333333134651184\n",
            "Epoch  41  started\n",
            "Loss 0.5607070922851562 | Acc 0.8500000238418579 | Validation Loss 0.6024383902549744 | Validation Acc 0.8666666746139526\n",
            "Epoch  42  started\n",
            "Loss 0.5556012988090515 | Acc 0.875 | Validation Loss 0.5941715836524963 | Validation Acc 0.8666666746139526\n",
            "Epoch  43  started\n",
            "Loss 0.5423764586448669 | Acc 0.9166666865348816 | Validation Loss 0.5885413885116577 | Validation Acc 0.8333333134651184\n",
            "Epoch  44  started\n",
            "Loss 0.5406150221824646 | Acc 0.8500000238418579 | Validation Loss 0.5820185542106628 | Validation Acc 0.8333333134651184\n",
            "Epoch  45  started\n",
            "Loss 0.5272921919822693 | Acc 0.8666666746139526 | Validation Loss 0.5743299722671509 | Validation Acc 0.8666666746139526\n",
            "Epoch  46  started\n",
            "Loss 0.5268272161483765 | Acc 0.8916666507720947 | Validation Loss 0.5676256418228149 | Validation Acc 0.8666666746139526\n",
            "Epoch  47  started\n",
            "Loss 0.5215129256248474 | Acc 0.9083333611488342 | Validation Loss 0.5607572197914124 | Validation Acc 0.8666666746139526\n",
            "Epoch  48  started\n",
            "Loss 0.5115116834640503 | Acc 0.9166666865348816 | Validation Loss 0.5544559955596924 | Validation Acc 0.8666666746139526\n",
            "Epoch  49  started\n",
            "Loss 0.5042435526847839 | Acc 0.8999999761581421 | Validation Loss 0.5473048686981201 | Validation Acc 0.8666666746139526\n",
            "Epoch  50  started\n",
            "Loss 0.49907174706459045 | Acc 0.9333333373069763 | Validation Loss 0.5425470471382141 | Validation Acc 0.8666666746139526\n",
            "Epoch  51  started\n",
            "Loss 0.4879213869571686 | Acc 0.9083333611488342 | Validation Loss 0.5364876389503479 | Validation Acc 0.8666666746139526\n",
            "Epoch  52  started\n",
            "Loss 0.4829750955104828 | Acc 0.9333333373069763 | Validation Loss 0.5318840742111206 | Validation Acc 0.8666666746139526\n",
            "Epoch  53  started\n",
            "Loss 0.48465555906295776 | Acc 0.875 | Validation Loss 0.5236888527870178 | Validation Acc 0.8666666746139526\n",
            "Epoch  54  started\n",
            "Loss 0.4802672266960144 | Acc 0.9083333611488342 | Validation Loss 0.518281102180481 | Validation Acc 0.8999999761581421\n",
            "Epoch  55  started\n",
            "Loss 0.4763724207878113 | Acc 0.925000011920929 | Validation Loss 0.5127612948417664 | Validation Acc 0.8999999761581421\n",
            "Epoch  56  started\n",
            "Loss 0.4670112133026123 | Acc 0.9750000238418579 | Validation Loss 0.5102502107620239 | Validation Acc 0.8666666746139526\n",
            "Epoch  57  started\n",
            "Loss 0.4579502046108246 | Acc 0.925000011920929 | Validation Loss 0.5043981075286865 | Validation Acc 0.8666666746139526\n",
            "Epoch  58  started\n",
            "Loss 0.45470958948135376 | Acc 0.9416666626930237 | Validation Loss 0.49862876534461975 | Validation Acc 0.8666666746139526\n",
            "Epoch  59  started\n",
            "Loss 0.4522610902786255 | Acc 0.9166666865348816 | Validation Loss 0.4934701919555664 | Validation Acc 0.8999999761581421\n",
            "Epoch  60  started\n",
            "Loss 0.4365074336528778 | Acc 0.949999988079071 | Validation Loss 0.4898584187030792 | Validation Acc 0.8666666746139526\n",
            "Epoch  61  started\n",
            "Loss 0.4411563277244568 | Acc 0.9416666626930237 | Validation Loss 0.4849129319190979 | Validation Acc 0.8666666746139526\n",
            "Epoch  62  started\n",
            "Loss 0.4348475933074951 | Acc 0.949999988079071 | Validation Loss 0.4787527024745941 | Validation Acc 0.8999999761581421\n",
            "Epoch  63  started\n",
            "Loss 0.43128296732902527 | Acc 0.9416666626930237 | Validation Loss 0.4734070897102356 | Validation Acc 0.8999999761581421\n",
            "Epoch  64  started\n",
            "Loss 0.4275560677051544 | Acc 0.9583333134651184 | Validation Loss 0.46857112646102905 | Validation Acc 0.8999999761581421\n",
            "Epoch  65  started\n",
            "Loss 0.4184623062610626 | Acc 0.949999988079071 | Validation Loss 0.4651222229003906 | Validation Acc 0.8666666746139526\n",
            "Epoch  66  started\n",
            "Loss 0.4215753972530365 | Acc 0.925000011920929 | Validation Loss 0.46110400557518005 | Validation Acc 0.8666666746139526\n",
            "Epoch  67  started\n",
            "Loss 0.4131234586238861 | Acc 0.949999988079071 | Validation Loss 0.456735759973526 | Validation Acc 0.8999999761581421\n",
            "Epoch  68  started\n",
            "Loss 0.4110230803489685 | Acc 0.9333333373069763 | Validation Loss 0.4518750011920929 | Validation Acc 0.8999999761581421\n",
            "Epoch  69  started\n",
            "Loss 0.4044573903083801 | Acc 0.9666666388511658 | Validation Loss 0.44820263981819153 | Validation Acc 0.8999999761581421\n",
            "Epoch  70  started\n",
            "Loss 0.4010048806667328 | Acc 0.9666666388511658 | Validation Loss 0.44624295830726624 | Validation Acc 0.8999999761581421\n",
            "Epoch  71  started\n",
            "Loss 0.40022239089012146 | Acc 0.9333333373069763 | Validation Loss 0.4407562017440796 | Validation Acc 0.8999999761581421\n",
            "Epoch  72  started\n",
            "Loss 0.3893453776836395 | Acc 0.9666666388511658 | Validation Loss 0.4374179244041443 | Validation Acc 0.8999999761581421\n",
            "Epoch  73  started\n",
            "Loss 0.39176538586616516 | Acc 0.949999988079071 | Validation Loss 0.4334045648574829 | Validation Acc 0.8999999761581421\n",
            "Epoch  74  started\n",
            "Loss 0.38177746534347534 | Acc 0.9833333492279053 | Validation Loss 0.4324225187301636 | Validation Acc 0.8999999761581421\n",
            "Epoch  75  started\n",
            "Loss 0.38033854961395264 | Acc 0.9583333134651184 | Validation Loss 0.4298063814640045 | Validation Acc 0.8666666746139526\n",
            "Epoch  76  started\n",
            "Loss 0.3815096914768219 | Acc 0.9583333134651184 | Validation Loss 0.4252934455871582 | Validation Acc 0.8999999761581421\n",
            "Epoch  77  started\n",
            "Loss 0.38070133328437805 | Acc 0.9583333134651184 | Validation Loss 0.42303404211997986 | Validation Acc 0.8666666746139526\n",
            "Epoch  78  started\n",
            "Loss 0.37381279468536377 | Acc 0.949999988079071 | Validation Loss 0.41619613766670227 | Validation Acc 0.8999999761581421\n",
            "Epoch  79  started\n",
            "Loss 0.37162354588508606 | Acc 0.9750000238418579 | Validation Loss 0.41294029355049133 | Validation Acc 0.8999999761581421\n",
            "Epoch  80  started\n",
            "Loss 0.3645034730434418 | Acc 0.9750000238418579 | Validation Loss 0.4096122682094574 | Validation Acc 0.8999999761581421\n",
            "Epoch  81  started\n",
            "Loss 0.36478132009506226 | Acc 0.9666666388511658 | Validation Loss 0.40724244713783264 | Validation Acc 0.8999999761581421\n",
            "Epoch  82  started\n",
            "Loss 0.36496469378471375 | Acc 0.9333333373069763 | Validation Loss 0.40326252579689026 | Validation Acc 0.8999999761581421\n",
            "Epoch  83  started\n",
            "Loss 0.357805997133255 | Acc 0.9750000238418579 | Validation Loss 0.4001584053039551 | Validation Acc 0.8999999761581421\n",
            "Epoch  84  started\n",
            "Loss 0.352550208568573 | Acc 0.9750000238418579 | Validation Loss 0.3989133834838867 | Validation Acc 0.8999999761581421\n",
            "Epoch  85  started\n",
            "Loss 0.3480379283428192 | Acc 0.9666666388511658 | Validation Loss 0.3963499963283539 | Validation Acc 0.8999999761581421\n",
            "Epoch  86  started\n",
            "Loss 0.34833383560180664 | Acc 0.9583333134651184 | Validation Loss 0.3915115296840668 | Validation Acc 0.8999999761581421\n",
            "Epoch  87  started\n",
            "Loss 0.3458941578865051 | Acc 0.9750000238418579 | Validation Loss 0.3881988227367401 | Validation Acc 0.9333333373069763\n",
            "Epoch  88  started\n",
            "Loss 0.34282419085502625 | Acc 0.9833333492279053 | Validation Loss 0.38751378655433655 | Validation Acc 0.8999999761581421\n",
            "Epoch  89  started\n",
            "Loss 0.3426826000213623 | Acc 0.9666666388511658 | Validation Loss 0.3850373923778534 | Validation Acc 0.8999999761581421\n",
            "Epoch  90  started\n",
            "Loss 0.3337905704975128 | Acc 0.9666666388511658 | Validation Loss 0.38129326701164246 | Validation Acc 0.8999999761581421\n",
            "Epoch  91  started\n",
            "Loss 0.33455416560173035 | Acc 0.9666666388511658 | Validation Loss 0.3778988718986511 | Validation Acc 0.8999999761581421\n",
            "Epoch  92  started\n",
            "Loss 0.3297898769378662 | Acc 0.9750000238418579 | Validation Loss 0.3746868968009949 | Validation Acc 0.9333333373069763\n",
            "Epoch  93  started\n",
            "Loss 0.3281283676624298 | Acc 0.9833333492279053 | Validation Loss 0.37296798825263977 | Validation Acc 0.8999999761581421\n",
            "Epoch  94  started\n",
            "Loss 0.32121655344963074 | Acc 0.9833333492279053 | Validation Loss 0.37184765934944153 | Validation Acc 0.8999999761581421\n",
            "Epoch  95  started\n",
            "Loss 0.32638853788375854 | Acc 0.9750000238418579 | Validation Loss 0.36718109250068665 | Validation Acc 0.8999999761581421\n",
            "Epoch  96  started\n",
            "Loss 0.31850892305374146 | Acc 0.9750000238418579 | Validation Loss 0.3674951195716858 | Validation Acc 0.8999999761581421\n",
            "Epoch  97  started\n",
            "Loss 0.3134602904319763 | Acc 0.9750000238418579 | Validation Loss 0.3645072877407074 | Validation Acc 0.8999999761581421\n",
            "Epoch  98  started\n",
            "Loss 0.3157139718532562 | Acc 0.9666666388511658 | Validation Loss 0.3597899377346039 | Validation Acc 0.8999999761581421\n",
            "Epoch  99  started\n",
            "Loss 0.3118745684623718 | Acc 0.9833333492279053 | Validation Loss 0.35959476232528687 | Validation Acc 0.8999999761581421\n",
            "Epoch  100  started\n",
            "Loss 0.30787187814712524 | Acc 0.9833333492279053 | Validation Loss 0.3587194085121155 | Validation Acc 0.8999999761581421\n",
            "Epoch  101  started\n",
            "Loss 0.3076668083667755 | Acc 0.9666666388511658 | Validation Loss 0.3535515367984772 | Validation Acc 0.8999999761581421\n",
            "Epoch  102  started\n",
            "Loss 0.3023555874824524 | Acc 0.9750000238418579 | Validation Loss 0.35429975390434265 | Validation Acc 0.8999999761581421\n",
            "Epoch  103  started\n",
            "Loss 0.2984696328639984 | Acc 0.9583333134651184 | Validation Loss 0.3475363850593567 | Validation Acc 0.8999999761581421\n",
            "Epoch  104  started\n",
            "Loss 0.296566903591156 | Acc 0.9750000238418579 | Validation Loss 0.34474560618400574 | Validation Acc 0.9333333373069763\n",
            "Epoch  105  started\n",
            "Loss 0.30048036575317383 | Acc 0.9750000238418579 | Validation Loss 0.3419100344181061 | Validation Acc 0.9333333373069763\n",
            "Epoch  106  started\n",
            "Loss 0.2934046983718872 | Acc 0.9833333492279053 | Validation Loss 0.34143689274787903 | Validation Acc 0.8999999761581421\n",
            "Epoch  107  started\n",
            "Loss 0.28926199674606323 | Acc 0.9833333492279053 | Validation Loss 0.3406318128108978 | Validation Acc 0.8999999761581421\n",
            "Epoch  108  started\n",
            "Loss 0.2869955003261566 | Acc 0.9833333492279053 | Validation Loss 0.33895784616470337 | Validation Acc 0.8999999761581421\n",
            "Epoch  109  started\n",
            "Loss 0.2823728024959564 | Acc 0.9833333492279053 | Validation Loss 0.3389343023300171 | Validation Acc 0.8999999761581421\n",
            "Epoch  110  started\n",
            "Loss 0.2846934497356415 | Acc 0.9750000238418579 | Validation Loss 0.3333940804004669 | Validation Acc 0.8999999761581421\n",
            "Epoch  111  started\n",
            "Loss 0.2850497364997864 | Acc 0.9750000238418579 | Validation Loss 0.3311622738838196 | Validation Acc 0.8999999761581421\n",
            "Epoch  112  started\n",
            "Loss 0.2764623463153839 | Acc 0.9833333492279053 | Validation Loss 0.3294205069541931 | Validation Acc 0.8999999761581421\n",
            "Epoch  113  started\n",
            "Loss 0.2774391174316406 | Acc 0.9750000238418579 | Validation Loss 0.323922336101532 | Validation Acc 0.9333333373069763\n",
            "Epoch  114  started\n",
            "Loss 0.2755354642868042 | Acc 0.9833333492279053 | Validation Loss 0.32450905442237854 | Validation Acc 0.8999999761581421\n",
            "Epoch  115  started\n",
            "Loss 0.28211280703544617 | Acc 0.9833333492279053 | Validation Loss 0.3234955668449402 | Validation Acc 0.8999999761581421\n",
            "Epoch  116  started\n",
            "Loss 0.26791128516197205 | Acc 0.9833333492279053 | Validation Loss 0.3238406479358673 | Validation Acc 0.8999999761581421\n",
            "Epoch  117  started\n",
            "Loss 0.26775455474853516 | Acc 0.9750000238418579 | Validation Loss 0.31909531354904175 | Validation Acc 0.8999999761581421\n",
            "Epoch  118  started\n",
            "Loss 0.26342180371284485 | Acc 0.9833333492279053 | Validation Loss 0.31801146268844604 | Validation Acc 0.8999999761581421\n",
            "Epoch  119  started\n",
            "Loss 0.2665961980819702 | Acc 0.9833333492279053 | Validation Loss 0.31404945254325867 | Validation Acc 0.9333333373069763\n",
            "Epoch  120  started\n",
            "Loss 0.26290491223335266 | Acc 0.9833333492279053 | Validation Loss 0.3114706575870514 | Validation Acc 0.9333333373069763\n",
            "Epoch  121  started\n",
            "Loss 0.26216328144073486 | Acc 0.9833333492279053 | Validation Loss 0.3102860748767853 | Validation Acc 0.9333333373069763\n",
            "Epoch  122  started\n",
            "Loss 0.2587950825691223 | Acc 0.9916666746139526 | Validation Loss 0.3118758797645569 | Validation Acc 0.8999999761581421\n",
            "Epoch  123  started\n",
            "Loss 0.25132209062576294 | Acc 0.9750000238418579 | Validation Loss 0.3114713430404663 | Validation Acc 0.8999999761581421\n",
            "Epoch  124  started\n",
            "Loss 0.25654980540275574 | Acc 0.9750000238418579 | Validation Loss 0.3073926568031311 | Validation Acc 0.8999999761581421\n",
            "Epoch  125  started\n",
            "Loss 0.25352734327316284 | Acc 0.9833333492279053 | Validation Loss 0.3031807243824005 | Validation Acc 0.9333333373069763\n",
            "Epoch  126  started\n",
            "Loss 0.2498244345188141 | Acc 0.9833333492279053 | Validation Loss 0.30119088292121887 | Validation Acc 0.9333333373069763\n",
            "Epoch  127  started\n",
            "Loss 0.24918732047080994 | Acc 0.9833333492279053 | Validation Loss 0.30121737718582153 | Validation Acc 0.9333333373069763\n",
            "Epoch  128  started\n",
            "Loss 0.24637958407402039 | Acc 0.9833333492279053 | Validation Loss 0.29780152440071106 | Validation Acc 0.9333333373069763\n",
            "Epoch  129  started\n",
            "Loss 0.24441707134246826 | Acc 0.9750000238418579 | Validation Loss 0.29428112506866455 | Validation Acc 0.9333333373069763\n",
            "Epoch  130  started\n",
            "Loss 0.24242310225963593 | Acc 0.9916666746139526 | Validation Loss 0.29576200246810913 | Validation Acc 0.9333333373069763\n",
            "Epoch  131  started\n",
            "Loss 0.24261623620986938 | Acc 0.9833333492279053 | Validation Loss 0.29561322927474976 | Validation Acc 0.8999999761581421\n",
            "Epoch  132  started\n",
            "Loss 0.2399551272392273 | Acc 0.9833333492279053 | Validation Loss 0.2952081263065338 | Validation Acc 0.8999999761581421\n",
            "Epoch  133  started\n",
            "Loss 0.23807884752750397 | Acc 0.9750000238418579 | Validation Loss 0.28794631361961365 | Validation Acc 0.9333333373069763\n",
            "Epoch  134  started\n",
            "Loss 0.2378670573234558 | Acc 0.9833333492279053 | Validation Loss 0.2856117784976959 | Validation Acc 0.9333333373069763\n",
            "Epoch  135  started\n",
            "Loss 0.2331015020608902 | Acc 0.9916666746139526 | Validation Loss 0.28821244835853577 | Validation Acc 0.9333333373069763\n",
            "Epoch  136  started\n",
            "Loss 0.2301940619945526 | Acc 0.9833333492279053 | Validation Loss 0.2857169210910797 | Validation Acc 0.9333333373069763\n",
            "Epoch  137  started\n",
            "Loss 0.22718669474124908 | Acc 0.9833333492279053 | Validation Loss 0.2853720784187317 | Validation Acc 0.9333333373069763\n",
            "Epoch  138  started\n",
            "Loss 0.22832411527633667 | Acc 0.9833333492279053 | Validation Loss 0.2832642197608948 | Validation Acc 0.9333333373069763\n",
            "Epoch  139  started\n",
            "Loss 0.2270505130290985 | Acc 0.9833333492279053 | Validation Loss 0.2776404321193695 | Validation Acc 0.9333333373069763\n",
            "Epoch  140  started\n",
            "Loss 0.22545991837978363 | Acc 0.9833333492279053 | Validation Loss 0.2759568989276886 | Validation Acc 0.9333333373069763\n",
            "Epoch  141  started\n",
            "Loss 0.22018678486347198 | Acc 0.9833333492279053 | Validation Loss 0.2762696146965027 | Validation Acc 0.9333333373069763\n",
            "Epoch  142  started\n",
            "Loss 0.22327262163162231 | Acc 0.9833333492279053 | Validation Loss 0.27422112226486206 | Validation Acc 0.9333333373069763\n",
            "Epoch  143  started\n",
            "Loss 0.22164514660835266 | Acc 0.9833333492279053 | Validation Loss 0.2772615849971771 | Validation Acc 0.8999999761581421\n",
            "Epoch  144  started\n",
            "Loss 0.21804319322109222 | Acc 0.9833333492279053 | Validation Loss 0.2744746208190918 | Validation Acc 0.9333333373069763\n",
            "Epoch  145  started\n",
            "Loss 0.2169569581747055 | Acc 0.9833333492279053 | Validation Loss 0.27435174584388733 | Validation Acc 0.8999999761581421\n",
            "Epoch  146  started\n",
            "Loss 0.217254638671875 | Acc 0.9833333492279053 | Validation Loss 0.27194133400917053 | Validation Acc 0.9333333373069763\n",
            "Epoch  147  started\n",
            "Loss 0.2125246226787567 | Acc 0.9833333492279053 | Validation Loss 0.2660166919231415 | Validation Acc 0.9333333373069763\n",
            "Epoch  148  started\n",
            "Loss 0.2097683846950531 | Acc 0.9833333492279053 | Validation Loss 0.26922136545181274 | Validation Acc 0.9333333373069763\n",
            "Epoch  149  started\n",
            "Loss 0.2165924608707428 | Acc 0.9833333492279053 | Validation Loss 0.2643369734287262 | Validation Acc 0.9333333373069763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK-T_qBGQ9el",
        "colab_type": "text"
      },
      "source": [
        "After about 100 or so itertions, the val acc seems to be plateau. Of course these are not the most ideal hyperparametrs or the metrics, but for now they'll do the job.\n",
        "\n",
        "We can plot the metrics to see how the training went"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7NmTdh4Ji6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5f7be2cb-08dc-43dc-b958-d452870b8e4c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(metrics['loss'], c='r')\n",
        "plt.plot(metrics['val_loss'], c='b')\n",
        "plt.show()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzjmXJDygO+dckvj/SnoHSL5QQ1wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as0tGWoEKq7X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "2df0d674-e6b9-458a-b7dd-a34f8e7e7e9e"
      },
      "source": [
        "plt.plot(metrics['acc'], c='r')\n",
        "plt.plot(metrics['val_acc'], c='b')\n",
        "plt.show()"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNQQKp0Rh-Y",
        "colab_type": "text"
      },
      "source": [
        "By the way, you can also use the tf dataset instance with all its pipeline components with keras' fit method also."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E9qUT81K3iK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def model_fn2():\n",
        "#   model_in = layers.Input((4,))\n",
        "#   a = layers.Dense(12, activation='relu', kernel_regularizer= tf.keras.regularizers.l2(l=0.001))(model_in)\n",
        "#   logits = layers.Dense(3, activation='softmax')(a)\n",
        "#   model= models.Model(model_in, logits)\n",
        "#   return model\n",
        "\n",
        "# model2 = model_fn2()\n",
        "# model2.compile(optimizer='adam',loss='SparseCategoricalCrossentropy', metrics=['acc'])\n",
        "# a = model2.fit(ds_train, epochs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY2gV62lR1Tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}